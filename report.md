## **项目报告：基于深度强化学习的选课策略优化研究**

### **项目简介**

本项目旨在研究并应用深度强化学习（DRL）技术，以解决一个模拟的、基于竞价机制的大学课程选择问题。我们构建了一个动态的、多竞争者的选课环境，学生需要在有限的积分预算内，为多门具有不同个人偏好的课程出价，以期在满足最低选课数量要求的基础上，最大化个人满意度。

为达成此目标，我们实现了四种具有代表性的DRL算法：深度Q网络（DQN）、REINFORCE、演员-评论家（A2C）以及深度确定性策略梯度（DDPG）。通过在一个统一的平台上对这些算法进行训练和评估，我们系统地比较了它们在学习效率、收敛稳定性及最终策略质量上的表现。

实验结果表明，A2C和DDPG等基于演员-评论家框架的算法，在本任务中展现出卓越的性能，能够学习到高效且合理的资源分配策略。本报告详细阐述了环境建模、算法设计、实验过程与结果分析，验证了DRL在解决复杂资源分配与策略决策问题上的有效性与潜力。

-----

### **1. 引言**

#### **1.1 项目背景**

南方科技大学的选课和竞价模型很像。假设22级有 $a$ 名本科生，共有 $b$ 门课，每个学生有 $100$ 个积分。对于每个学生，对于他感兴趣的课，他可以投若干积分。越感兴趣投的分越多。一门课的总学生容量有 $30$ 人，确定选课学生的方式是按照他们的积分来排序（如果最后几名学生投的积分相同，则他们都选不上）。课可以不选满学生，但是学生每学期至少要上 $3$ 门课，最多选 $7$ 门课。为了简化问题，每个学生有 $x$ 门"最喜欢"的课，y 门“中等喜好的课”, $z$ 门“不想上但为了学分不得不上的课”(假设 $x<y<z,x+y+z=b$)。

#### **1.2 问题定义**

本项目构建了一个模拟选课环境，其核心要素如下：

  * **主体**: 100名学生，共享20门课程资源。
  * **资源**: 每名学生拥有100个初始积分用于竞价。
  * **偏好**: 课程对每个学生分为“最喜欢”（4门）、“中等喜好”（8门）和“不喜欢”（8门）三类。
  * **机制**:
      * 每门课程容量为30人，按出价高低录取。
      * 出价相同且处于录取边缘的学生均会落选。
      * 学生最终成功选上的课程数需满足`[3, 7]`的区间要求。
  * **目标**: 制定一个最优的积分分配策略，在满足选课数量下限的前提下，最大化选到偏好课程所带来的“满意度”。

#### **1.3 项目目标**

  * **环境构建**: 开发一个 `CourseSelectionEnv` 模拟器。
  * **算法实现**: 实现并调试DQN, REINFORCE, A2C, DDPG四种RL算法。
  * **性能对比**: 在同一环境下对所有算法进行训练，并从学习速度、收敛性和最终奖励等多个维度进行横向比较。

-----

### **2. 方法论**

#### **2.1 环境建模 (MDP)**

我们将该问题建模为一个马尔可夫决策过程 (Markov Decision Process, MDP)，其关键元素定义如下：

  * **状态 (State)**: 为了让Agent做出有根据的决策，在为第`i`门课出价时，状态`s`被设计为包含以下信息的向量：

    1.  **剩余积分比例**: `remaining_points / initial_points`
    2.  **当前课程偏好**: 对课程类型（最喜欢/中等/不喜欢）的One-Hot编码。
    3.  **剩余决策比例**: `(total_courses - i) / total_courses`
        这个归一化的状态表示，让Agent能感知自身资源、当前决策的重要性和未来的决策压力。

  * **动作 (Action)**:

      * 对于DQN, REINFORCE, A2C，动作空间被**离散化**为一组固定的出价档位：`[0, 5, 10, ..., 50]`。
      * 对于DDPG，动作空间为**连续的**，允许Agent输出`[0, 50]`区间内的任意浮点数作为出价。

  * **奖励 (Reward)**:
    我们设计了一个稀疏的、在回合结束时才计算的最终奖励函数，以引导Agent达成长期目标：

    1.  **生存惩罚**: 若最终选课数 \< 3，奖励为 `-100`。
    2.  **满意度奖励**: 根据选上课程的偏好类型给予不同奖励（最喜欢:`+15`, 中等:`+7`, 不喜欢:`+2`）。
    3.  **效率惩罚**: 每有一个出价但落选的课程，奖励 `-1`，以鼓励有效投资。

#### **2.2 算法实现**

  * **DQN**: 实现了一个带有经验回放池和独立目标网络的Q学习智能体。它通过学习Q函数来间接指导离散动作的选择，是价值学习的基石。
  * **REINFORCE**: 实现了一个经典的策略梯度智能体。它直接对策略本身进行参数化，并使用蒙特卡洛方法（完整的选课回合）来估计策略梯度，指导网络更新。
  * **A2C (演员-评论家)**: 实现了一个包含Actor（策略网络）和Critic（价值网络）的复合智能体。Critic负责评估状态的价值，以降低REINFORCE中的高方差梯度估计，从而提供更稳定、高效的学习信号来指导Actor。
  * **DDPG**: 作为A2C在连续动作空间上的拓展，其Actor网络输出一个确定的动作值。同样采用了经验回放和目标网络技术来保证离线学习的稳定性。

#### **2.3 实验设置**

  * **环境参数**: 严格遵循1.2节的定义。
  * **训练回合**: DQN, A2C, DDPG设置为5000回合；REINFORCE因其高方差和较低的样本效率，设置为10000回合以保证充分学习。
  * **硬件平台**: Python 3.x, PyTorch。代码内置设备检测逻辑，优先使用CUDA进行GPU加速，否则使用CPU。

-----

### **3. 结果与分析**

#### **3.1 算法性能比较**

通过运行所有算法并将其百次滑动平均奖励绘制在同一图表中，我们观察到以下趋势：

  * **学习效率**: A2C和DDPG的奖励曲线在训练早期便迅速攀升，展现出最高的样本效率。DQN次之，其学习曲线稳定上升。REINFORCE由于其固有的高方差，学习启动最慢，曲线波动也最大。
  * **最终性能**: 在训练结束时，A2C和DDPG收敛到了最高的平均奖励水平，表明它们学到了更优的策略。DQN的性能紧随其后。在有限的回合内，REINFORCE的性能显著低于其他三者。
  * **稳定性**: DQN、A2C和DDPG的学习曲线相对平滑，而REINFORCE的曲线则呈现出较大的抖动，这与算法的理论特性相符。

![learning_curve](assets/comparison_chart2.png)

#### **3.2 策略分析**

通过分析A2C和DDPG等表现优异的Agent在训练结束后的决策行为，我们发现其学到的策略高度符合人类直觉和最优决策逻辑：

  * **差异化出价**: Agent会对“最喜欢”的课程果断投入高额积分（如20-40分），对“中等喜好”的课程投入中等积分（如5-15分），而对“不喜欢”的课程则几乎不出价（0分）。
  * **预算管理**: 在选课流程的初期，若遇到心仪课程，Agent敢于出高价。随着积分的消耗，其出价行为会变得愈发谨慎。
  * **目标导向**: Agent的行为明确地以“先保证选上3门课”为隐性前提，在此基础上再追求满意度最大化。

-----

### **4. 结论与展望**

#### **4.1 结论**

本项目成功地通过深度强化学习解决了复杂的选课策略问题。研究表明：

1.  DRL能够有效地为此类带约束的资源分配问题学习到高质量的决策策略。
2.  在四种被测算法中，基于**演员-评论家（Actor-Critic）框架的A2C和DDPG算法**综合表现最佳，在学习效率和最终性能上均优于经典的DQN和REINFORCE算法。这证明了在决策过程中引入状态价值基线（baseline）对于稳定学习和加速收敛至关重要。
3.  无论是离散还是连续的动作空间，DRL都能找到相应的解决方案，其中DDPG在连续空间下的精细决策能力展现了其独特的优势。

#### **4.2 项目局限性**

  * **单智能体视角**: 本项目从单个学生的视角进行优化，而将其余99名学生建模为固定的随机策略。这简化了问题，但与真实世界中所有学生均为智能博弈体的场景有一定差距。
  * **环境简化**: 未考虑课程先修关系、上课时间冲突等更复杂的现实约束。

#### **4.3 未来展望**

  * **多智能体强化学习 (MARL)**: 下一步的核心工作是将此问题扩展为MARL框架，让所有学生Agent共同学习和进化，研究竞争环境下的均衡策略。
  * **模型与算法优化**: 探索更先进的DRL算法（如PPO, SAC），并对现有模型的超参数进行系统性调优。
  * **丰富环境动态**: 在模拟器中加入更多现实世界的约束，使其更具挑战性和应用价值。